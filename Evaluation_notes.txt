### Evaluation & Metrics

## Dataset
- Financial News Sentiment Dataset (Kaggle)
- Classes: Positive, Negative, Neutral
- Evaluation performed on a stratified test split

---

### Zero-Shot LLM Evaluation
The LLM was evaluated in a zero-shot setting without task-specific fine-tuning.

**Metrics (sample-based):**
- Accuracy: 0.82%
- Confusion Matrix: [[0,0,0],
				[0,0,0],
				[0,0,0]]

**Observations:**
- Neutral headlines are the most challenging due to lack of explicit financial signals.
- Forward-looking statements often introduce ambiguity.

---

### Classical ML Evaluation
A TF-IDF + Logistic Regression model was used as the classical baseline.

**Metrics:**
- Accuracy: 75%
- Macro F1-score: 69%
- Confusion Matrix: [[69,33,19]
				[21,487,67]
				[14,92,166]]

**Observations:**
- Performs well on clearly positive or negative statements.
- Struggles with context-dependent neutral cases.

---

### ML vs LLM Comparison
- LLM captures semantic context better in narrative headlines.
- ML performs more consistently on structurally similar text.
- Disagreement analysis highlights complementary strengths.

---

### Error Analysis (LLM-assisted)
Error analysis was performed **only on genuine LLM misclassifications**.

**Key insights:**
- Mixed financial signals (growth + risk) confuse sentiment boundaries.
- Headlines lacking numerical indicators are prone to misclassification.
- Historical vs speculative news affects sentiment interpretation.

---

### Limitations
- Short headlines provide limited context.
- No temporal market data is used.
- Zero-shot LLM output may vary across providers.

---

### Guardrails & Reliability
- Output constrained to {Positive, Negative, Neutral}
- Deterministic decoding (temperature = 0)
- Error analysis skipped when no misclassifications exist