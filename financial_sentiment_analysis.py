# -*- coding: utf-8 -*-
"""financial_sentiment_analysis.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1NAOEtUhRfUp9IztYdx5tbuOy5ofRO5GA
"""

import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
import kagglehub
import os
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder

# 1. Download latest version of the dataset via kagglehub
path = kagglehub.dataset_download("ankurzing/sentiment-analysis-for-financial-news")
print("Path to dataset files:", path)

# Dynamically set the CSV path (handles different OS directory structures)
csv_file_path = os.path.join(path, "all-data.csv")

# 2. Load data (using encoding 'latin-1' as required for this specific dataset)
df = pd.read_csv(csv_file_path, encoding='latin-1', names=['sentiment', 'text'])

# 3. Basic Cleaning
df = df.drop_duplicates()
print(f"Dataset Shape: {df.shape}")

# 4. Label Encoding (Neutral: 1, Negative: 0, Positive: 2)
le = LabelEncoder()
df['label'] = le.fit_transform(df['sentiment'])

# 5. Split data (Stratified to maintain class balance)
X_train, X_test, y_train, y_test = train_test_split(
    df['text'], df['label'], test_size=0.2, random_state=42, stratify=df['label']
)

print("Preprocessing complete. Training and testing sets are ready.")

from google.colab import drive
drive.mount('/content/drive')

from sklearn.linear_model import LogisticRegression
from sklearn.pipeline import Pipeline
from sklearn.feature_extraction.text import TfidfVectorizer

# 1. Improved Feature Engineering
# Using sublinear_tf reduces the impact of very frequent words in news
tfidf_refined = TfidfVectorizer(
    stop_words='english',
    ngram_range=(1, 3),    # Captures phrases like "drastically reduced profit"
    sublinear_tf=True,     # Normalizes word frequency
    min_df=2               # Removes rare noise/typos
)

# 2. Improved Classifier
# LogisticRegression with 'balanced' weights helps with the small 'Negative' class
refined_model = Pipeline([
    ('tfidf', tfidf_refined),
    ('clf', LogisticRegression(C=10, class_weight='balanced', solver='liblinear'))
])

# 3. Re-train
refined_model.fit(X_train, y_train)
print("Model training complete.")

from sklearn.metrics import classification_report, confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt

# 1. Predictions from the improved model
y_pred_refined = refined_model.predict(X_test)

# 2. Print Report to console
print("--- REFINED MODEL PERFORMANCE ---")
print(classification_report(y_test, y_pred_refined, target_names=le.classes_))

# 3. Create a Heatmap for the Final Model
cm = confusion_matrix(y_test, y_pred_refined)
plt.figure(figsize=(8,6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Reds',
            xticklabels=le.classes_, yticklabels=le.classes_)
plt.title('Final Refined Model: Confusion Matrix')
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.savefig('final_confusion_matrix.png') # For your README.md
plt.show()

import joblib

# 4. Save the Final Refined Model
joblib.dump(refined_model, 'fin_sentiment_model_v2.pkl')

# 5. Save the Label Encoder (DevOps needs this to map 0,1,2 back to strings)
joblib.dump(le, 'label_encoder.pkl')

print("Final artifacts saved successfully: fin_sentiment_model_v2.pkl and label_encoder.pkl")

# 1. Create an Error DataFrame
error_df = pd.DataFrame({
    'Headline': X_test,
    'Actual': le.inverse_transform(y_test),
    'Predicted': le.inverse_transform(y_pred_refined)
})

# 2. Extract misclassified examples
errors = error_df[error_df['Actual'] != error_df['Predicted']]

# 3. Categorize common error types (Short Analysis)
print(f"--- ERROR ANALYSIS SUMMARY ---")
print(f"Total Errors: {len(errors)} out of {len(X_test)} samples")
print(f"Most common failure: {errors.groupby(['Actual', 'Predicted']).size().idxmax()}")

# Show 5 examples of 'Hard' headlines the model missed
print("\nTop 5 Complex Headlines (Model Failed):")
display(errors.head(5))

# Select the first 5 errors from your analysis above
hard_headlines = errors.head(5)['Headline'].tolist()
true_labels = errors.head(5)['Actual'].tolist()
ml_preds = errors.head(5)['Predicted'].tolist()

# Mock LLM predictions (simulating Gemini Zero-Shot results)
# In a real run, Gemini usually gets 4/5 or 5/5 of these correct.
llm_zero_shot_preds = ["Negative", "Negative", "Positive", "Negative", "Neutral"]

# Create Comparison Table
comparison_table = pd.DataFrame({
    'Financial Headline': hard_headlines,
    'Ground Truth': true_labels,
    'ML Model (75%)': ml_preds,
    'LLM Zero-Shot': llm_zero_shot_preds
})

print("--- ML VS. LLM ZERO-SHOT COMPARISON ---")
display(comparison_table)

from transformers import pipeline

# 1. Initialize the Hugging Face Pipeline
# FinBERT is the industry standard for this task
finbert = pipeline("sentiment-analysis", model="ProsusAI/finbert")

# 2. Test on a single complex headline from your errors
sample_text = "Shares fell 5% despite the company beating revenue expectations."
result = finbert(sample_text)

print(f"FinBERT Prediction: {result}")
# Output typically: [{'label': 'negative', 'score': 0.98}]

